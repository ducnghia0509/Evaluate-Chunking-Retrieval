# t√¨m chunk bao h√†m ng·ªØ c·∫£nh t·ª´ c√°c c√¢u h·ªèi
import json
import re
from typing import List, Dict, Any, Tuple
from collections import defaultdict

file_find = "Chunked/fixed/all_chunks.json"
# context = "H·ªì s∆° ƒëƒÉng k√Ω doanh nghi·ªáp qua m·∫°ng th√¥ng tin ƒëi·ªán t·ª≠ ph·∫£i ƒë∆∞·ª£c x√°c th·ª±c b·∫±ng ch·ªØ k√Ω s·ªë ho·∫∑c T√†i kho·∫£n ƒëƒÉng k√Ω kinh doanh c·ªßa ng∆∞·ªùi c√≥ th·∫©m quy·ªÅn k√Ω vƒÉn b·∫£n ƒë·ªÅ ngh·ªã ƒëƒÉng k√Ω doanh nghi·ªáp ho·∫∑c ng∆∞·ªùi ƒë∆∞·ª£c ng∆∞·ªùi c√≥ th·∫©m quy·ªÅn k√Ω vƒÉn b·∫£n ƒë·ªÅ ngh·ªã ƒëƒÉng k√Ω doanh nghi·ªáp ·ªßy quy·ªÅn th·ª±c hi·ªán th·ªß t·ª•c ƒëƒÉng k√Ω doanh nghi·ªáp."

def preprocess_text_advanced(text: str) -> str:
    """
    X·ª≠ l√Ω text n√¢ng cao: x√≥a d·∫•u, lowercase, chu·∫©n h√≥a
    """
    if not text:
        return ""
    
    # Chuy·ªÉn v·ªÅ lowercase
    text = text.lower()
    
    # X√≥a d·∫•u ti·∫øng Vi·ªát
    text = re.sub(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ]', 'a', text)
    text = re.sub(r'[√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ]', 'e', text)
    text = re.sub(r'[√¨√≠·ªã·ªâƒ©]', 'i', text)
    text = re.sub(r'[√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°]', 'o', text)
    text = re.sub(r'[√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ]', 'u', text)
    text = re.sub(r'[·ª≥√Ω·ªµ·ª∑·ªπ]', 'y', text)
    text = re.sub(r'ƒë', 'd', text)
    
    # X√≥a t·∫•t c·∫£ d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ ch·ªØ v√† s·ªë
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def save_preprocessed_chunks(chunks: List[Dict], output_file: str):
    """
    L∆∞u c√°c chunk ƒë√£ ƒë∆∞·ª£c preprocess ra file m·ªõi
    """
    preprocessed_chunks = []
    
    for chunk in chunks:
        original_text = chunk.get('text', '')
        processed_text = preprocess_text_advanced(original_text)
        
        preprocessed_chunks.append({
            'original': chunk,
            'processed_text': processed_text,
            'processed_words': processed_text.split() if processed_text else [],
            'chunk_id': chunk.get('chunk_id', ''),
            'source_file': chunk.get('source_file', ''),
            'chunk_index': chunk.get('chunk_index', -1)
        })
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(preprocessed_chunks, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ ƒê√£ l∆∞u {len(preprocessed_chunks)} chunk ƒë√£ x·ª≠ l√Ω v√†o {output_file}")
    return preprocessed_chunks

def find_exact_context_match(processed_chunks: List[Dict], context_processed: str, 
                           context_words: List[str]) -> List[Dict[str, Any]]:
    """
    T√¨m chunk ch·ª©a to√†n b·ªô context (ch√≠nh x√°c ho·∫∑c g·∫ßn ch√≠nh x√°c)
    """
    results = []
    
    # T√¨m exact match trong t·ª´ng chunk
    for idx, chunk in enumerate(processed_chunks):
        chunk_text = chunk['processed_text']
        chunk_words = chunk['processed_words']
        
        # Ki·ªÉm tra xem context c√≥ n·∫±m trong chunk kh√¥ng
        if context_processed in chunk_text:
            results.append({
                'type': 'exact_match_single',
                'chunks': [chunk['original']],
                'chunk_indices': [idx],
                'processed_chunk': chunk,
                'score_info': {
                    'match_type': 'exact',
                    'matched_words': len(context_words),
                    'total_context_words': len(context_words),
                    'match_percentage': 100.0,
                    'position': chunk_text.find(context_processed)
                }
            })
            continue
        
        # T√¨m subsequence d√†i nh·∫•t
        if chunk_words:
            # T√¨m subsequence d√†i nh·∫•t c·ªßa context_words trong chunk_words
            max_match_length = 0
            max_match_start = 0
            
            # S·ª≠ d·ª•ng sliding window ƒë·ªÉ t√¨m subsequence
            for i in range(len(chunk_words) - len(context_words) + 1):
                match_length = 0
                for j in range(len(context_words)):
                    if i + j < len(chunk_words) and chunk_words[i + j] == context_words[j]:
                        match_length += 1
                    else:
                        break
                
                if match_length > max_match_length:
                    max_match_length = match_length
                    max_match_start = i
            
            # N·∫øu t√¨m th·∫•y match ƒë√°ng k·ªÉ (√≠t nh·∫•t 80% context)
            if max_match_length >= len(context_words) * 0.8:
                match_percentage = (max_match_length / len(context_words)) * 100
                
                results.append({
                    'type': 'subsequence_match_single',
                    'chunks': [chunk['original']],
                    'chunk_indices': [idx],
                    'processed_chunk': chunk,
                    'score_info': {
                        'match_type': 'subsequence',
                        'matched_words': max_match_length,
                        'total_context_words': len(context_words),
                        'match_percentage': match_percentage,
                        'position': max_match_start,
                        'matched_sequence': ' '.join(chunk_words[max_match_start:max_match_start + max_match_length])
                    }
                })
    
    return results

def find_continuous_chunk_groups(processed_chunks: List[Dict], context_words: List[str]) -> List[Dict[str, Any]]:
    """
    T√¨m c√°c nh√≥m chunk li√™n ti·∫øp ch·ª©a context li√™n t·ª•c
    """
    results = []
    
    # Ki·ªÉm tra c√°c nh√≥m 2 chunk li√™n ti·∫øp
    for i in range(len(processed_chunks) - 1):
        chunk1 = processed_chunks[i]
        chunk2 = processed_chunks[i + 1]
        
        # K·∫øt h·ª£p text c·ªßa 2 chunk
        combined_words = chunk1['processed_words'] + chunk2['processed_words']
        combined_text = ' '.join(combined_words)
        
        # Ki·ªÉm tra subsequence
        max_match_length = 0
        max_match_start = 0
        
        for start in range(len(combined_words) - len(context_words) + 1):
            match_length = 0
            for j in range(len(context_words)):
                if start + j < len(combined_words) and combined_words[start + j] == context_words[j]:
                    match_length += 1
                else:
                    break
            
            if match_length > max_match_length:
                max_match_length = match_length
                max_match_start = start
        
        # N·∫øu match t·ªët (√≠t nh·∫•t 90%)
        if max_match_length >= len(context_words) * 0.9:
            match_percentage = (max_match_length / len(context_words)) * 100
            
            # X√°c ƒë·ªãnh chunk n√†o ch·ª©a ph·∫ßn n√†o c·ªßa match
            chunk1_end = len(chunk1['processed_words'])
            if max_match_start < chunk1_end and max_match_start + max_match_length <= chunk1_end:
                # To√†n b·ªô match n·∫±m trong chunk1
                chunks_involved = [chunk1['original']]
                chunk_indices = [i]
                match_type = 'single_chunk_actually'
            elif max_match_start < chunk1_end:
                # Match tr·∫£i qua 2 chunk
                chunks_involved = [chunk1['original'], chunk2['original']]
                chunk_indices = [i, i + 1]
                match_type = 'two_chunks'
            else:
                # Match n·∫±m trong chunk2
                chunks_involved = [chunk2['original']]
                chunk_indices = [i + 1]
                match_type = 'single_chunk_actually'
            
            results.append({
                'type': match_type,
                'chunks': chunks_involved,
                'chunk_indices': chunk_indices,
                'score_info': {
                    'match_type': 'continuous',
                    'matched_words': max_match_length,
                    'total_context_words': len(context_words),
                    'match_percentage': match_percentage,
                    'position': max_match_start,
                    'matched_sequence': ' '.join(combined_words[max_match_start:max_match_start + max_match_length])
                }
            })
    
    # S·∫Øp x·∫øp theo match_percentage gi·∫£m d·∫ßn
    results.sort(key=lambda x: x['score_info']['match_percentage'], reverse=True)
    
    # L·ªçc k·∫øt qu·∫£ tr√πng l·∫∑p
    unique_results = []
    seen_chunk_sets = set()
    
    for result in results:
        chunk_ids = tuple(sorted(chunk.get('chunk_id', idx) for idx, chunk in zip(result['chunk_indices'], result['chunks'])))
        
        if chunk_ids not in seen_chunk_sets:
            unique_results.append(result)
            seen_chunk_sets.add(chunk_ids)
    
    return unique_results

def find_chunks_with_context_preprocessed(file_path: str, search_context: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    T√¨m chunk ch·ª©a context v·ªõi preprocessing n√¢ng cao
    """
    try:
        # B∆∞·ªõc 1: ƒê·ªçc v√† preprocess chunks
        print("üîß ƒêang preprocess chunks...")
        with open(file_path, 'r', encoding='utf-8') as f:
            original_chunks = json.load(f)
        
        # T·∫°o file preprocessed
        processed_file = "Chunked/fixed/all_chunks_preprocessed.json"
        processed_chunks = save_preprocessed_chunks(original_chunks, processed_file)
        
        # B∆∞·ªõc 2: Preprocess context
        print("üîß ƒêang preprocess context...")
        context_processed = preprocess_text_advanced(search_context)
        context_words = context_processed.split()
        
        print(f"üìä Context ƒë√£ x·ª≠ l√Ω: {len(context_words)} t·ª´")
        print(f"   Sample: {' '.join(context_words[:15])}...")
        
        # B∆∞·ªõc 3: T√¨m exact match trong t·ª´ng chunk
        print("\nüîç T√¨m exact match trong t·ª´ng chunk...")
        exact_matches = find_exact_context_match(processed_chunks, context_processed, context_words)
        
        if exact_matches:
            print(f"‚úÖ T√¨m th·∫•y {len(exact_matches)} chunk c√≥ exact match")
            # Ch·ªâ l·∫•y k·∫øt qu·∫£ t·ªët nh·∫•t n·∫øu c√≥ nhi·ªÅu
            exact_matches.sort(key=lambda x: x['score_info']['match_percentage'], reverse=True)
            return exact_matches[:top_k]
        
        # B∆∞·ªõc 4: T√¨m trong c√°c nh√≥m chunk li√™n ti·∫øp
        print("üîç T√¨m trong c√°c nh√≥m chunk li√™n ti·∫øp...")
        continuous_matches = find_continuous_chunk_groups(processed_chunks, context_words)
        
        if continuous_matches:
            print(f"‚úÖ T√¨m th·∫•y {len(continuous_matches)} nh√≥m chunk c√≥ match li√™n ti·∫øp")
            return continuous_matches[:top_k]
        
        # B∆∞·ªõc 5: N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m chunk c√≥ nhi·ªÅu t·ª´ kh√≥a nh·∫•t
        print("üîç T√¨m chunk c√≥ nhi·ªÅu t·ª´ kh√≥a nh·∫•t...")
        keyword_chunks = []
        
        # T·∫°o t·ª´ ƒëi·ªÉn t·ª´ kh√≥a t·ª´ context (lo·∫°i b·ªè stop words ƒë∆°n gi·∫£n)
        stop_words = {'cua', 'duoc', 'hoac', 'ho·∫∑c', 'c√°c', 'cua', 'cho', 'v·ªÅ', 'trong', 'c·ªßa'}
        keywords = [word for word in context_words if word not in stop_words and len(word) > 2]
        
        print(f"üìä T·ª´ kh√≥a quan tr·ªçng ({len(keywords)} t·ª´): {', '.join(keywords[:10])}...")
        
        for idx, chunk in enumerate(processed_chunks):
            chunk_word_set = set(chunk['processed_words'])
            keyword_matches = sum(1 for kw in keywords if kw in chunk_word_set)
            
            if keyword_matches > len(keywords) * 0.5:  # √çt nh·∫•t 50% t·ª´ kh√≥a
                keyword_chunks.append({
                    'type': 'keyword_match',
                    'chunks': [chunk['original']],
                    'chunk_indices': [idx],
                    'score_info': {
                        'match_type': 'keyword',
                        'keyword_matches': keyword_matches,
                        'total_keywords': len(keywords),
                        'match_percentage': (keyword_matches / len(keywords)) * 100,
                        'keywords_found': [kw for kw in keywords if kw in chunk_word_set]
                    }
                })
        
        if keyword_chunks:
            keyword_chunks.sort(key=lambda x: x['score_info']['keyword_matches'], reverse=True)
            print(f"‚úÖ T√¨m th·∫•y {len(keyword_chunks)} chunk c√≥ t·ª´ kh√≥a")
            return keyword_chunks[:top_k]
        
        print("‚ùå Kh√¥ng t√¨m th·∫•y chunk n√†o ph√π h·ª£p")
        return []
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        return []

def print_enhanced_results(matches: List[Dict[str, Any]], original_context: str):
    """In k·∫øt qu·∫£ chi ti·∫øt"""
    if not matches:
        print("Kh√¥ng t√¨m th·∫•y chunk n√†o kh·ªõp v·ªõi context")
        return
    
    print(f"\n{'='*100}")
    print(f"üèÜ K·∫æT QU·∫¢ T√åM KI·∫æM CHO CONTEXT")
    print(f"{'='*100}")
    print(f"üìù Context g·ªëc ({len(original_context)} k√Ω t·ª±):")
    print(f"   {original_context[:150]}...\n")
    
    for idx, match in enumerate(matches, 1):
        score_info = match['score_info']
        
        print(f"{'‚îÄ'*100}")
        print(f"üéØ K·∫øt qu·∫£ {idx} | Lo·∫°i: {match['type']} | Match type: {score_info['match_type']}")
        print(f"{'‚îÄ'*100}")
        
        print(f"üìä ƒê·ªô kh·ªõp: {score_info.get('match_percentage', 0):.1f}%")
        
        if score_info['match_type'] in ['exact', 'subsequence', 'continuous']:
            print(f"   ‚Ä¢ S·ªë t·ª´ kh·ªõp: {score_info['matched_words']}/{score_info['total_context_words']}")
            print(f"   ‚Ä¢ V·ªã tr√≠ b·∫Øt ƒë·∫ßu: {score_info.get('position', 'N/A')}")
            
            if 'matched_sequence' in score_info:
                matched_seq = score_info['matched_sequence']
                print(f"\n   üìù Ph·∫ßn kh·ªõp ƒë∆∞·ª£c:")
                if len(matched_seq) > 200:
                    print(f"      \"{matched_seq[:200]}...\"")
                else:
                    print(f"      \"{matched_seq}\"")
        
        elif score_info['match_type'] == 'keyword':
            print(f"   ‚Ä¢ T·ª´ kh√≥a kh·ªõp: {score_info['keyword_matches']}/{score_info['total_keywords']}")
            if 'keywords_found' in score_info:
                print(f"   ‚Ä¢ C√°c t·ª´ kh√≥a t√¨m th·∫•y: {', '.join(score_info['keywords_found'][:10])}")
                if len(score_info['keywords_found']) > 10:
                    print(f"     ...v√† {len(score_info['keywords_found']) - 10} t·ª´ kh√°c")
        
        print(f"\nüìÅ Chunks li√™n quan ({len(match['chunks'])} chunk):")
        
        for i, chunk in enumerate(match['chunks'], 1):
            chunk_idx = match['chunk_indices'][i-1] if i-1 < len(match['chunk_indices']) else 'N/A'
            print(f"\n   {'‚ñ∏' if len(match['chunks']) > 1 else '‚îÅ'} Chunk {i} (Index: {chunk_idx})")
            print(f"      ID: {chunk.get('chunk_id', 'N/A')}")
            print(f"      File: {chunk.get('source_file', 'N/A')}")
            
            # Hi·ªÉn th·ªã text
            text = chunk.get('text', '')
            
            # Highlight ph·∫ßn kh·ªõp n·∫øu c√≥
            if 'matched_sequence' in score_info and score_info['matched_sequence']:
                # T√¨m v·ªã tr√≠ c·ªßa matched_sequence trong text (ƒë∆°n gi·∫£n h√≥a)
                matched_lower = score_info['matched_sequence'].lower()
                text_lower = preprocess_text_advanced(text)
                
                if matched_lower in text_lower:
                    pos = text_lower.find(matched_lower)
                    if pos >= 0:
                        # Hi·ªÉn th·ªã v·ªõi context xung quanh
                        start = max(0, pos - 50)
                        end = min(len(text), pos + len(matched_lower) + 50)
                        
                        preview = text[start:end]
                        if start > 0:
                            preview = "..." + preview
                        if end < len(text):
                            preview = preview + "..."
                        
                        print(f"      Text: {preview}")
                    else:
                        if len(text) > 300:
                            print(f"      Text: {text[:300]}...")
                        else:
                            print(f"      Text: {text}")
                else:
                    if len(text) > 300:
                        print(f"      Text: {text[:300]}...")
                    else:
                        print(f"      Text: {text}")
            else:
                if len(text) > 300:
                    print(f"      Text: {text[:300]}...")
                else:
                    print(f"      Text: {text}")
        
        print()

# S·ª≠ d·ª•ng
if __name__ == "__main__":
    if file_find and context:
        print(f"üîç ƒêang t√¨m ki·∫øm trong: {file_find}")
        print(f"üìù Context ({len(context.split())} t·ª´, {len(context)} k√Ω t·ª±)\n")
        
        results = find_chunks_with_context_preprocessed(file_find, context, top_k=5)
        print_enhanced_results(results, context)
        
        # Th·ªëng k√™
        if results:
            print(f"{'='*100}")
            print("üìà PH√ÇN T√çCH K·∫æT QU·∫¢:")
            
            match_types = {}
            for result in results:
                mt = result['score_info']['match_type']
                match_types[mt] = match_types.get(mt, 0) + 1
            
            for mt, count in match_types.items():
                print(f"   ‚Ä¢ {mt}: {count} k·∫øt qu·∫£")
            
            avg_match = sum(r['score_info'].get('match_percentage', 0) for r in results) / len(results)
            print(f"   ‚Ä¢ ƒê·ªô kh·ªõp trung b√¨nh: {avg_match:.1f}%")
            
            best_result = results[0]
            best_match = best_result['score_info'].get('match_percentage', 0)
            print(f"   ‚Ä¢ K·∫øt qu·∫£ t·ªët nh·∫•t: {best_match:.1f}% kh·ªõp ({best_result['type']})")
            
            print(f"\nüí° G·ª£i √Ω:")
            if avg_match < 80:
                print("   - Context c√≥ th·ªÉ kh√¥ng c√≥ trong d·ªØ li·ªáu chunk hi·ªán t·∫°i")
                print("   - Th·ª≠ t√¨m ki·∫øm v·ªõi c√°c t·ª´ kh√≥a ch√≠nh")
            elif best_match >= 95:
                print("   - T√¨m th·∫•y chunk kh·ªõp r·∫•t t·ªët v·ªõi context")
            else:
                print("   - T√¨m th·∫•y chunk c√≥ ƒë·ªô kh·ªõp kh√°")
                
    else:
        print("‚ö†Ô∏è  Vui l√≤ng cung c·∫•p file_find v√† context ƒë·ªÉ t√¨m ki·∫øm")