# file n√†y ƒë·ªÉ t·∫°o c√°c c√¢u h·ªèi d·ª±a tr√™n m·∫´u ƒë∆∞·ª£c gen t·ª´ deepseek, copy v√†o file "fix"

import json
import re
from typing import List, Dict, Any
from find_chunk import find_chunks_with_context_preprocessed

def parse_fix_file(file_path: str) -> List[Dict[str, Any]]:
    """
    Parse file fix ƒë·ªÉ l·∫•y c√°c c√¢u h·ªèi v√† ng·ªØ c·∫£nh theo t·ª´ng category
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Split theo category
    sections = re.split(r'={50,}\n([A-Z\s]+)\n={50,}', content)
    
    results = []
    current_category = None
    
    for i in range(1, len(sections), 2):
        category_name = sections[i].strip()
        category_content = sections[i+1] if i+1 < len(sections) else ""
        
        # Map category name
        if "DOC" in category_name and "LAW" not in category_name:
            current_category = "doc"
        elif "DOCUMENT" in category_name and "LAW" in category_name:
            current_category = "document_law"
        elif "FICTION" in category_name:
            current_category = "fiction"
        else:
            continue
        
        # Parse questions trong category n√†y
        questions = parse_questions(category_content, current_category)
        results.extend(questions)
    
    return results

def parse_questions(content: str, category: str) -> List[Dict[str, Any]]:
    """
    Parse c√°c c√¢u h·ªèi v√† ng·ªØ c·∫£nh t·ª´ content c·ªßa m·ªôt category
    ƒê·ªçc t·ª´ng d√≤ng ƒë·ªÉ x·ª≠ l√Ω ch√≠nh x√°c c√¢u h·ªèi c√≥ ch·ª©a d·∫•u ngo·∫∑c k√©p
    """
    questions = []
    lines = content.split('\n')
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        # T√¨m d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng "C√¢u h·ªèi:"
        if line.startswith('C√¢u h·ªèi:'):
            # L·∫•y ph·∫ßn sau "C√¢u h·ªèi:", lo·∫°i b·ªè d·∫•u " ƒë·∫ßu v√† cu·ªëi
            question_part = line[len('C√¢u h·ªèi:'):].strip()
            
            # T√¨m d·∫•u " ƒë·∫ßu ti√™n v√† cu·ªëi c√πng
            if question_part.startswith('"') and question_part.endswith('"'):
                question_text = question_part[1:-1].strip()
            else:
                # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ d·∫•u " ƒë·∫ßu/cu·ªëi ho·∫∑c format kh√°c
                i += 1
                continue
            
            i += 1
            
            # T√¨m d√≤ng "Ng·ªØ c·∫£nh li√™n quan:"
            contexts = []
            while i < len(lines):
                line = lines[i].strip()
                
                if line.startswith('Ng·ªØ c·∫£nh li√™n quan:'):
                    # Ki·ªÉm tra xem ngay sau "Ng·ªØ c·∫£nh li√™n quan:" c√≥ n·ªôi dung kh√¥ng
                    context_part = line[len('Ng·ªØ c·∫£nh li√™n quan:'):].strip()
                    
                    if context_part.startswith('"') and context_part.endswith('"'):
                        # Context n·∫±m ngay tr√™n c√πng d√≤ng
                        contexts.append(context_part[1:-1].strip())
                    
                    i += 1
                    
                    # ƒê·ªçc c√°c d√≤ng ti·∫øp theo cho ƒë·∫øn khi g·∫∑p d√≤ng tr·ªëng ho·∫∑c "C√¢u h·ªèi:" m·ªõi
                    while i < len(lines):
                        line = lines[i].strip()
                        
                        # N·∫øu g·∫∑p d√≤ng tr·ªëng ho·∫∑c "C√¢u h·ªèi:" m·ªõi ho·∫∑c "===" => k·∫øt th√∫c
                        if not line or line.startswith('C√¢u h·ªèi:') or line.startswith('==='):
                            break
                        
                        # N·∫øu d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng ", ƒë√¢y l√† m·ªôt ng·ªØ c·∫£nh
                        if line.startswith('"') and line.endswith('"'):
                            contexts.append(line[1:-1].strip())
                        
                        i += 1
                    
                    break
                
                i += 1
            
            # Th√™m c√¢u h·ªèi v√†o danh s√°ch n·∫øu c√≥ ng·ªØ c·∫£nh
            if contexts:
                questions.append({
                    "question": question_text,
                    "contexts": contexts,
                    "category": category
                })
        else:
            i += 1
    
    return questions

def find_chunks_for_contexts(types: str,contexts: List[str], category: str) -> List[Dict[str, Any]]:
    """
    T√¨m chunk ph√π h·ª£p cho M·ªñI ƒëo·∫°n context ri√™ng bi·ªát
    Tr·∫£ v·ªÅ danh s√°ch c√°c chunk_id duy nh·∫•t
    """
    # X√°c ƒë·ªãnh file chunks d·ª±a tr√™n category
    simple = "/all_chunks"
    if types == "parent_child":
        simple = "/all_child_chunks"
    chunk_file_map = {
        "doc": "Chunked/"+types+ simple + ".json",
        "document_law": "Chunked/"+types+ simple + ".json",
        "fiction": "Chunked/"+types+ simple + ".json"
    }
    chunk_file_path = "Chunked/"+types+ simple + ".json"
    chunk_file = chunk_file_map.get(category, chunk_file_path)

    # Dictionary ƒë·ªÉ l∆∞u chunks duy nh·∫•t v√† match_percentage cao nh·∫•t
    found_chunks = {}
    
    # T√¨m chunks cho t·ª´ng ƒëo·∫°n context
    for ctx_idx, context in enumerate(contexts, 1):
        print(f"      - ƒêo·∫°n ng·ªØ c·∫£nh {ctx_idx}/{len(contexts)} ({len(context)} k√Ω t·ª±)...")
        
        # T√¨m NHI·ªÄU chunks (top_k=5) thay v√¨ ch·ªâ 1
        results = find_chunks_with_context_preprocessed(chunk_file, context, top_k=3)
        
        if results and len(results) > 0:
            # Duy·ªát qua T·∫§T C·∫¢ k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c
            for result in results:
                chunks = result.get('chunks', [])
                match_percentage = result.get('score_info', {}).get('match_percentage', 0)
                
                # Ch·ªâ l·∫•y chunks c√≥ match >= 70%
                if match_percentage >= 70:
                    for chunk in chunks:
                        chunk_id = chunk.get('chunk_id', '')
                        
                        # N·∫øu chunk_id ch∆∞a c√≥ ho·∫∑c c√≥ match_percentage cao h∆°n, c·∫≠p nh·∫≠t
                        if chunk_id not in found_chunks or found_chunks[chunk_id]['match_percentage'] < match_percentage:
                            found_chunks[chunk_id] = {
                                'match_percentage': match_percentage
                            }
                        
                        print(f"         ‚úì T√¨m th·∫•y: {chunk_id} (match: {match_percentage:.1f}%)")
    
    # Chuy·ªÉn sang format output
    result_chunks = []
    for chunk_id, info in found_chunks.items():
        match_percentage = info['match_percentage']
        
        # X√°c ƒë·ªãnh relevance d·ª±a tr√™n match percentage
        if match_percentage >= 90:
            relevance = "high"
        elif match_percentage >= 70:
            relevance = "medium"
        else:
            relevance = "low"
        
        result_chunks.append({
            "chunk_id": chunk_id,
            "relevance": relevance
        })
    
    return result_chunks

def create_evaluation_dataset(types):
    """
    T·∫°o file evaluation dataset
    """
    print("üîç ƒêang parse file fix...")
    questions = parse_fix_file("fix")
    
    print(f"‚úÖ ƒê√£ parse {len(questions)} c√¢u h·ªèi")
    
    evaluation_data = []
    
    for idx, q in enumerate(questions, 1):
        print(f"\n[{idx}/{len(questions)}] ƒêang x·ª≠ l√Ω c√¢u h·ªèi: {q['question'][:50]}...")
        print(f"   Category: {q['category']}")
        print(f"   S·ªë ƒëo·∫°n ng·ªØ c·∫£nh: {len(q['contexts'])}")
        
        # T√¨m chunks cho T·∫§T C·∫¢ c√°c ƒëo·∫°n contexts
        found_chunks = find_chunks_for_contexts(types, q['contexts'], q['category'])
        
        if found_chunks:
            print(f"   ‚úÖ T√¨m th·∫•y {len(found_chunks)} chunk(s)")
            
            evaluation_data.append({
                "id": f"{q['category']}_{idx}",
                "category": q['category'],
                "question": q['question'],
                "query": q['question'],  # Gi·ªØ nguy√™n nh∆∞ y√™u c·∫ßu
                "relevant_chunks": found_chunks
            })
        else:
            print(f"   ‚ùå Kh√¥ng t√¨m th·∫•y chunk ph√π h·ª£p")
            # V·∫´n th√™m v√†o nh∆∞ng kh√¥ng c√≥ chunk
            evaluation_data.append({
                "id": f"{q['category']}_{idx}",
                "category": q['category'],
                "question": q['question'],
                "query": q['question'],
                "relevant_chunks": []
            })
    
    # L∆∞u file JSON
    output_file = types + "_evaluation_dataset.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*80}")
    print(f"‚úÖ ƒê√É T·∫†O FILE EVALUATION DATASET")
    print(f"{'='*80}")
    print(f"üìÅ File: {output_file}")
    print(f"üìä T·ªïng s·ªë c√¢u h·ªèi: {len(evaluation_data)}")
    
    # Th·ªëng k√™
    categories = {}
    with_chunks = 0
    without_chunks = 0
    multi_chunks = 0  # S·ªë c√¢u h·ªèi c√≥ nhi·ªÅu h∆°n 1 chunk
    
    for item in evaluation_data:
        cat = item['category']
        categories[cat] = categories.get(cat, 0) + 1
        
        num_chunks = len(item['relevant_chunks'])
        if num_chunks > 0:
            with_chunks += 1
            if num_chunks > 1:
                multi_chunks += 1
        else:
            without_chunks += 1
    
    print(f"\nüìà TH·ªêNG K√ä:")
    print(f"   ‚Ä¢ C√¢u h·ªèi c√≥ chunk: {with_chunks}")
    print(f"   ‚Ä¢ C√¢u h·ªèi c√≥ NHI·ªÄU H∆†NS 1 chunk: {multi_chunks}")
    print(f"   ‚Ä¢ C√¢u h·ªèi kh√¥ng c√≥ chunk: {without_chunks}")
    print(f"\n   Theo category:")
    for cat, count in categories.items():
        print(f"   ‚Ä¢ {cat}: {count} c√¢u h·ªèi")
    
    return evaluation_data

if __name__ == "__main__":
    # typess = ["fixed", "hierarchical", "semantic", "structure_paragraph"]
    typess = ["parent_child"]
    for types in typess:
        print(f"=============BEGIN {types}=============")
        create_evaluation_dataset(types)
        print(f"=============END {types}=============")

